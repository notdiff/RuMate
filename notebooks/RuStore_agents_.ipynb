{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcK6-So2HTfU"
      },
      "source": [
        "# import lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGmiO42iHTfX"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import langchain\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import TextLoader\n",
        "import os\n",
        "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "import warnings\n",
        "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.retrievers import TFIDFRetriever\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "from langchain_community.llms import YandexGPT\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.tools import BaseTool\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiQlA4ZuHTfY"
      },
      "source": [
        "# YandexGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqMP0IrdHTfY"
      },
      "outputs": [],
      "source": [
        "def yagpt_request(question, context):\n",
        "    url = \"https://llm.api.cloud.yandex.net/foundationModels/v1/completion\"\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": \"Api-Key ----\"\n",
        "    }\n",
        "\n",
        "    ya_prompt = {\n",
        "        \"modelUri\": \"gpt://b1g72uajlds114mlufqi/yandexgpt/latest\",\n",
        "        \"completionOptions\": {\n",
        "            \"stream\": False,\n",
        "            \"temperature\": 0.6,\n",
        "            \"maxTokens\": \"1024\"\n",
        "        },\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"text\": context\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"text\": question\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, headers=headers, json=ya_prompt)\n",
        "    result = eval(response.text)\n",
        "\n",
        "    return result['result']['alternatives'][0]['message']['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "au5UvOgMHTfY",
        "outputId": "a5202142-1f9c-4b70-bd57-4cf0270c2589"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Я — Пётр Первый, и я человек, а не лев или пёс. Но я могу быть львом в том смысле, что я стремился изменить Россию и сделать её более сильной и современной, как царь зверей.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "yagpt_request('Ты Лев или пёс?', 'Ты — Пётр Первый')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcH3i2X_HTfZ"
      },
      "source": [
        "## Add YandexGPT to langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8lhyOwMHTfZ"
      },
      "outputs": [],
      "source": [
        "from typing import Any, Dict, Iterator, List, Mapping, Optional\n",
        "\n",
        "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
        "from langchain_core.language_models.llms import LLM\n",
        "from langchain_core.outputs import GenerationChunk\n",
        "\n",
        "\n",
        "class CustomLLM(LLM):\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        role: str = 'Ты — RuMate, сервис для ответа по документации RuStore, если вопрос не касается документации приложения (не связан с программированием, подключением подписок, выкладывание приложения и т.д), то стоит отвечать так: \"Извините, я бот отвечающий на вопросы по документации, уточните свой вопрос\"',\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        '''\n",
        "        if stop is not None:\n",
        "            raise ValueError(\"stop kwargs are not permitted.\")\n",
        "            '''\n",
        "        return yagpt_request(prompt, role)\n",
        "\n",
        "    def _stream(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> Iterator[GenerationChunk]:\n",
        "        \"\"\"Stream the LLM on the given prompt.\n",
        "\n",
        "        This method should be overridden by subclasses that support streaming.\n",
        "\n",
        "        If not implemented, the default behavior of calls to stream will be to\n",
        "        fallback to the non-streaming version of the model and return\n",
        "        the output as a single chunk.\n",
        "\n",
        "        Args:\n",
        "            prompt: The prompt to generate from.\n",
        "            stop: Stop words to use when generating. Model output is cut off at the\n",
        "                first occurrence of any of these substrings.\n",
        "            run_manager: Callback manager for the run.\n",
        "            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
        "                to the model provider API call.\n",
        "\n",
        "        Returns:\n",
        "            An iterator of GenerationChunks.\n",
        "        \"\"\"\n",
        "        for char in prompt[: self.n]:\n",
        "            chunk = GenerationChunk(text=char)\n",
        "            if run_manager:\n",
        "                run_manager.on_llm_new_token(chunk.text, chunk=chunk)\n",
        "\n",
        "            yield chunk\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Dict[str, Any]:\n",
        "        \"\"\"Return a dictionary of identifying parameters.\"\"\"\n",
        "        return {\n",
        "            # The model name allows users to specify custom token counting\n",
        "            # rules in LLM monitoring applications (e.g., in LangSmith users\n",
        "            # can provide per token pricing for their model and monitor\n",
        "            # costs for the given LLM.)\n",
        "            \"model_name\": \"CustomChatModel\",\n",
        "        }\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"Get the type of language model used by this chat model. Used for logging purposes only.\"\"\"\n",
        "        return \"custom\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jbsP9ZOHTfa",
        "outputId": "9c82d0a4-883b-4402-84c8-5b961a536ac3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mCustomLLM\u001b[0m\n",
            "Params: {'model_name': 'CustomChatModel'}\n"
          ]
        }
      ],
      "source": [
        "llm = CustomLLM()\n",
        "print(llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "5WHm4TVzHTfa",
        "outputId": "cf0cdf92-32d7-4a8f-b7f3-f81fe6f6ca95"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Я — RuMate, сервис для ответа по документации RuStore. Если ваш вопрос не касается документации приложения (не связан с программированием, подключением подписок, выкладыванием приложения и т. д.), то стоит уточнить свой вопрос.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "llm.invoke('Ты кто')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zuh56O2THTfa",
        "outputId": "24ce334a-aeb0-4323-cce2-3eba8be54331"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Извините, я бот, отвечающий на вопросы по документации. Уточните свой вопрос.\\n\\nWoof woof woof! Что вы хотели бы узнать о RuStore?',\n",
              " 'Извините, я бот, отвечающий на вопросы по документации. Уточните свой вопрос.\\n\\nmeow meow meow']"
            ]
          },
          "metadata": {},
          "execution_count": 335
        }
      ],
      "source": [
        "llm.batch([\"woof woof woof\", \"meow meow meow\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0op1N98HTfa"
      },
      "source": [
        "# RAG system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRGGmpbtHTfa"
      },
      "outputs": [],
      "source": [
        "from langchain.tools import BaseTool\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain.prompts import StringPromptTemplate\n",
        "from langchain.schema import AgentAction, AgentFinish, OutputParserException\n",
        "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
        "from typing import List, Union\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvIMcKmPHTfb"
      },
      "source": [
        "## Template for help Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xINs6-XLHTfb"
      },
      "outputs": [],
      "source": [
        "when_need_use_ask = open(r'/content/when_need_use_ask.txt','r').readline()\n",
        "when_need_use_fix_code = open(r'/content/when_need_use_fix_code.txt','r').readline()\n",
        "when_need_use_rag = open(r'/content/when_need_use_rag.txt','r').readline()\n",
        "when_need_use_simplify = open(r'/content/when_need_use_simplify.txt','r').readline()\n",
        "when_need_use_spliter = open(r'/content/when_need_use_spliter.txt','r').readline()\n",
        "\n",
        "when_need_use_ask_prompt = open(r'/content/AddAckerPrompt.txt','r').readline()\n",
        "when_need_use_fix_code_prompt = open(r'/content/FixCodePrompt.txt','r').readline()\n",
        "when_need_use_rag_prompt = open(r'/content/RagFuncPrompt.txt','r').readline()\n",
        "when_need_use_simplify_prompt = open(r'/content/SimplifyPrompt.txt','r').readline()\n",
        "when_need_use_spliter_prompt = open(r'/content/SplitToSimplePrompt.txt','r').readline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhD3W6pBHTfb"
      },
      "outputs": [],
      "source": [
        "class RagFuncTool(BaseTool):\n",
        "  name = \"RAG system\"\n",
        "  context = when_need_use_rag_prompt\n",
        "  description = when_need_use_rag\n",
        "  def _run(self, query: str, context=context):\n",
        "    return refactor_sim_search_res(query)\n",
        "\n",
        "class SplitToSimpleTool(BaseTool):\n",
        "  name = \"Split system\"\n",
        "  context = when_need_use_spliter_prompt\n",
        "  description = when_need_use_spliter\n",
        "  def _run(self, query: str, context=context):\n",
        "    return yagpt_request(query, context)\n",
        "\n",
        "class AddAckerTool(BaseTool):\n",
        "  name = \"Ask system\"\n",
        "  context = when_need_use_ask_prompt\n",
        "  description = when_need_use_ask\n",
        "  def _run(self, query: str, context=context):\n",
        "    return yagpt_request(query, context)\n",
        "\n",
        "class SimplifyTool(BaseTool):\n",
        "  name = \"Simple query system\"\n",
        "  context = when_need_use_simplify_prompt\n",
        "  description = when_need_use_simplify\n",
        "  def _run(self, query: str, context=context):\n",
        "    return yagpt_request(query, context)\n",
        "\n",
        "class FixCodeTool(BaseTool):\n",
        "  name = \"Fix Code system\"\n",
        "  context = when_need_use_fix_code_prompt\n",
        "  description = when_need_use_fix_code\n",
        "  def _run(self, query: str, context=context):\n",
        "    return yagpt_request(query, context)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0t2VJb7HTfb"
      },
      "outputs": [],
      "source": [
        "RagFunc = RagFuncTool()\n",
        "SplitToSimple = SplitToSimpleTool()\n",
        "AddAcker = AddAckerTool()\n",
        "Simplify = SimplifyTool()\n",
        "FixCode = FixCodeTool()\n",
        "\n",
        "tools = [\n",
        "    Tool(name=\"Search_RAG\", func=RagFunc._run, description=when_need_use_rag),\n",
        "    Tool(name=\"Spliter\", func=SplitToSimple._run, description=when_need_use_spliter),\n",
        "    Tool(name='simplifier', func=Simplify._run, description=when_need_use_simplify),\n",
        "    Tool(name='code_fixer', func=FixCode._run, description=when_need_use_fix_code)\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk_GG-rnHTfb"
      },
      "source": [
        "## Template for main Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFL_m0dqHTfb"
      },
      "outputs": [],
      "source": [
        "# Set up the base template\n",
        "template = \"\"\"Ты являешься ботом сервиса RuMate, который помогает отвечать на вопросы по документации сервиса для загрузки приложений. Ответь на следующие вопросы как можно лучше.\n",
        "В первую очередь старайся использовать функцию 'Search_RAG' для поиска релевантного ответа.\n",
        "Если не знаешь, то не нужно ничего придумывать.\n",
        "Если вопрос не по теме документации, то отвечай по типу \"уточните, пожалуйста, вопрос\".\n",
        "У тебя есть доступ к следующим инструментам:\n",
        "\n",
        "{tools}\n",
        "\n",
        "Use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think about what to do\n",
        "Action: the action to take, should be one of [{tool_names}]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original input question\n",
        "\n",
        "Начинай! Не забудьте говорить вежливо, чётко и ясно, когда будете давать свой окончательный ответ.\n",
        "\n",
        "Question: {input}\n",
        "{agent_scratchpad}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwdyDNtBHTfb"
      },
      "outputs": [],
      "source": [
        "# Set up a prompt template\n",
        "class CustomPromptTemplate(StringPromptTemplate):\n",
        "    # The template to use\n",
        "    template: str\n",
        "    # The list of tools available\n",
        "    tools: List[Tool]\n",
        "\n",
        "    def format(self, **kwargs) -> str:\n",
        "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
        "        # Format them in a particular way\n",
        "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
        "        thoughts = \"\"\n",
        "        for action, observation in intermediate_steps:\n",
        "            thoughts += action.log\n",
        "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
        "        # Set the agent_scratchpad variable to that value\n",
        "        kwargs[\"agent_scratchpad\"] = thoughts\n",
        "        # Create a tools variable from the list of tools provided\n",
        "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
        "        # Create a list of tool names for the tools provided\n",
        "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
        "        return self.template.format(**kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKR9Pl-lHTfb"
      },
      "outputs": [],
      "source": [
        "prompt = CustomPromptTemplate(\n",
        "    template=template,\n",
        "    tools=tools,\n",
        "    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n",
        "    # This includes the `intermediate_steps` variable because that is needed\n",
        "    input_variables=[\"input\", \"intermediate_steps\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlzubGxHHTfb"
      },
      "outputs": [],
      "source": [
        "class CustomOutputParser(AgentOutputParser):\n",
        "\n",
        "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
        "        # Check if agent should finish\n",
        "        if \"Final Answer:\" in llm_output:\n",
        "            return AgentFinish(\n",
        "                # Return values is generally always a dictionary with a single `output` key\n",
        "                # It is not recommended to try anything else at the moment :)\n",
        "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
        "                log=llm_output,\n",
        "            )\n",
        "        # Parse out the action and action input\n",
        "        regex = r\"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\n",
        "        match = re.search(regex, llm_output, re.DOTALL)\n",
        "        if not match:\n",
        "            raise OutputParserException(f\"Could not parse LLM output: `{llm_output}`\")\n",
        "        action = match.group(1).strip()\n",
        "        action_input = match.group(2)\n",
        "        # Return the action and action input\n",
        "        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoDO1hXhHTfc"
      },
      "outputs": [],
      "source": [
        "output_parser = CustomOutputParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baRNHjNdHTfc"
      },
      "source": [
        "## Integration agent to system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACDvNHfuHTfc"
      },
      "outputs": [],
      "source": [
        "# LLM chain consisting of the LLM and a prompt\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVkmDOcTHTfc"
      },
      "outputs": [],
      "source": [
        "\n",
        "tool_names = [tool.name for tool in tools]\n",
        "agent = LLMSingleActionAgent(\n",
        "    llm_chain=llm_chain,\n",
        "    output_parser=output_parser,\n",
        "    stop=[\"\\nObservation:\"],\n",
        "    allowed_tools=tool_names\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1Jg8cW6HTfc"
      },
      "outputs": [],
      "source": [
        "agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True,\n",
        "                                                    handle_parsing_errors=True, max_iterations=5, early_stopping_method='force',\n",
        "                                                    tags=sss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKNxR7LWHTfc"
      },
      "outputs": [],
      "source": [
        "answer = agent_executor.run(\"How to create аккаунит для разработчика?\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89oFYndm4NeH",
        "outputId": "30885df1-e75f-4a48-cc56-4660262dd100"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** Чтобы создать аккаунт для разработчика, нужно перейти на сайт RuStore, нажать кнопку регистрации, ввести данные, подтвердить адрес электронной почты и принять условия использования.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBj0caPZHTfc"
      },
      "source": [
        "## Connection with Retriever"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.1.2 sentence_transformers==2.2.2"
      ],
      "metadata": {
        "id": "jBE2JirzOyrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import BM25Retriever\n",
        "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
        "from langchain_community.chains.pebblo_retrieval.models import VectorDB\n",
        "from langchain_community.embeddings import HuggingFaceInstructEmbeddings"
      ],
      "metadata": {
        "id": "UJimXdHm71cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 7\n",
        "retriever_base =  BM25Retriever.from_documents(texts, k=k)\n",
        "retriever_advanced = vectordb.as_retriever(search_kwargs = {\"k\":  k, \"search_type\" : \"similarity\"})\n",
        "ensemble_retriever = EnsembleRetriever(retrievers=[retriever_base, retriever_advanced], weights=[0.5, 0.5], k=k)\n"
      ],
      "metadata": {
        "id": "OEaX5-Ze7z_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings.huggingface import HuggingFaceInstructEmbeddings\n",
        "import sentence_transformers\n",
        "import InstructorEmbedding\n",
        "import rank_bm25"
      ],
      "metadata": {
        "id": "iuYS2R3QHGxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkvnYuDBHTfc",
        "outputId": "a88a8e7d-5363-4d07-cbe3-78de416f0ada"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 386/386 [00:00<00:00, 2964.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 386 pages in total\n",
            "We have created 7866 chunks\n",
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n"
          ]
        }
      ],
      "source": [
        "loader = DirectoryLoader(\n",
        "    '/content/full_doc',\n",
        "    glob=\"./*.md\",\n",
        "    loader_cls=TextLoader,\n",
        "    show_progress=True,\n",
        "    use_multithreading=True\n",
        ")\n",
        "\n",
        "documents = loader.load()\n",
        "print(f'We have {len(documents)} pages in total')\n",
        "\n",
        "for page in range(len(documents)):\n",
        "    page_content = documents[page].page_content\n",
        "    page_content = page_content.replace('/help/', 'https://www.rustore.ru/help/')\n",
        "    documents[page].page_content = page_content\n",
        "\n",
        "headers_to_split_on = [\n",
        "    (\"#\", \"Header 1\"),\n",
        "]\n",
        "\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
        "\n",
        "all_splits = []\n",
        "\n",
        "for doc in documents:\n",
        "    md_header_splits = markdown_splitter.split_text(doc.page_content)\n",
        "    for i in range(len(md_header_splits)):\n",
        "        md_header_splits[i].metadata['source'] = doc.metadata['source']\n",
        "    all_splits.extend(md_header_splits)\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=800,\n",
        "    chunk_overlap=500\n",
        ")\n",
        "\n",
        "texts = text_splitter.split_documents(documents)\n",
        "print(f'We have created {len(texts)} chunks')\n",
        "\n",
        "embeddings = HuggingFaceInstructEmbeddings(\n",
        "    model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
        "    model_kwargs={\"device\": \"cpu\"}\n",
        ")\n",
        "\n",
        "vectordb = FAISS.load_local(\n",
        "    '/content/faiss_index_hp',  # from output folder\n",
        "    embeddings\n",
        ")\n",
        "\n",
        "k = 7\n",
        "retriever_base =  BM25Retriever.from_documents(texts, k=k)\n",
        "retriever_advanced = vectordb.as_retriever(search_kwargs = {\"k\":  k, \"search_type\" : \"similarity\"})\n",
        "ensemble_retriever = EnsembleRetriever(retrievers=[retriever_base, retriever_advanced], weights=[0.5, 0.5], k=k)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters.markdown import MarkdownHeaderTextSplitter\n",
        "import faiss\n",
        "from langchain.retrievers.bm25 import BM25Retriever"
      ],
      "metadata": {
        "id": "U-fdV7g4F6Ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def refactor_sim_search_res(question):\n",
        "    retriever_ans = ensemble_retriever.invoke(question, k=6)\n",
        "    retriever_ans = '\\n\\n'.join([i.page_content for i in retriever_ans])\n",
        "\n",
        "    context = tempulate.format(context=retriever_ans)\n",
        "\n",
        "    text = yagpt_request(question, context)\n",
        "    return text"
      ],
      "metadata": {
        "id": "OJeXpoQYLth8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}